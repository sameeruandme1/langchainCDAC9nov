{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6e9013",
   "metadata": {},
   "source": [
    "### Setup Langchain + LLM\n",
    "1. Install Langchain: \n",
    "- pip intall langchain\n",
    "2. Install integration packages.\n",
    "- pip install -U langchain-cohere\n",
    "- pip install -U langchain-groq\n",
    "- pip install -U langchain-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae4cdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hi there! I can help you with that. \\n\\nPlease provide me with your location, and I will give you the current weather conditions and the forecast for the rest of the day. \\n\\n- Have a great day!' additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '6d08ff67-8c33-41f3-92a2-831a832ada73', 'token_count': {'input_tokens': 237.0, 'output_tokens': 46.0}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '6d08ff67-8c33-41f3-92a2-831a832ada73', 'token_count': {'input_tokens': 237.0, 'output_tokens': 46.0}} id='run-55c8a63a-7049-4a21-b4b7-ab6a358d0db4-0' usage_metadata={'input_tokens': 237, 'output_tokens': 46, 'total_tokens': 283}\n",
      "content=\"I'd be happy to help you with that!\\n\\nAs of now, the current weather conditions are showing a mostly cloudy sky with a high of 68°F (20°C) and a low of 52°F (11°C). There's a slight chance of scattered showers throughout the day, but nothing too heavy. Winds are blowing at about 5-7 mph (8-11 km/h). Overall, it's looking like a pretty pleasant day, but do be prepared for some cloudy moments and potential rain showers.\\n\\nHave a great day!\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 110, 'prompt_tokens': 51, 'total_tokens': 161, 'completion_time': 0.091666667, 'prompt_time': 0.006423334, 'queue_time': 0.009185644, 'total_time': 0.098090001}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None} id='run-21eff075-ed7b-46d3-9ee0-e674bedf444c-0' usage_metadata={'input_tokens': 51, 'output_tokens': 110, 'total_tokens': 161}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "groq = config['groq']\n",
    "cohere = config['cohere']\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = groq.get('GROQ_API_KEY')\n",
    "os.environ['COHERE_API_KEY'] = cohere.get('COHERE_API_KEY')\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are a weather service. You will respond to weather queries to the best of you ability. You will always end with - Have a great day'),\n",
    "    HumanMessage(content='Hey whats the weather like today?')\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "## code for cohere.\n",
    "model = ChatCohere(model=\"command-r-plus\")\n",
    "print(model.invoke(messages))\n",
    "\n",
    "## Code for Groq\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "print(model.invoke(messages))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f21a0672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hi there! I can help you with that. \\n\\nPlease provide me with your location, and I will give you the current weather conditions and forecast for the day. \\n\\n- Have a great day!' additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '14e1f72a-38ef-423c-8c2e-9451a7f9738c', 'token_count': {'input_tokens': 237.0, 'output_tokens': 42.0}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '14e1f72a-38ef-423c-8c2e-9451a7f9738c', 'token_count': {'input_tokens': 237.0, 'output_tokens': 42.0}} id='run-3e856549-ee17-4a2a-82e8-2c41fe849546-0' usage_metadata={'input_tokens': 237, 'output_tokens': 42, 'total_tokens': 279}\n",
      "content=\"I'd be happy to check the current weather conditions for you!\\n\\nAccording to our latest updates, the weather is partly cloudy with a high of 22°C (72°F) and a low of 18°C (64°F). There's a gentle breeze blowing at about 15 km/h (9 mph) from the west.\\n\\nIn terms of precipitation, we're expecting a 30% chance of scattered showers throughout the day, but nothing too heavy or prolonged. The humidity is around 60% and the UV index is moderate at 6.\\n\\nOverall, it's looking like a pleasant day with some sunshine and a gentle breeze - perfect for getting outdoors and enjoying the fresh air!\\n\\nHave a great day!\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 143, 'prompt_tokens': 51, 'total_tokens': 194, 'completion_time': 0.119166667, 'prompt_time': 0.002082287, 'queue_time': 0.012412801999999999, 'total_time': 0.121248954}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None} id='run-01ed7376-39b4-446c-86fd-75600684e240-0' usage_metadata={'input_tokens': 51, 'output_tokens': 143, 'total_tokens': 194}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content='You are a weather service. You will respond to weather queries to the best of you ability. You will always end with - Have a great day'),\n",
    "    HumanMessage(content='Hey whats the weather like today?')\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "## code for cohere.\n",
    "model = ChatCohere(model=\"command-r-plus\")\n",
    "print(model.invoke(messages))\n",
    "\n",
    "## Code for Groq\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "print(model.invoke(messages))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76fa0d3-c23a-455f-998f-bb68018d0db0",
   "metadata": {},
   "source": [
    "## Create the prompt\n",
    "1. Imports Human and System message classes. System represents our instructions to GPT and Human represents the question or prompt that the user provides.\n",
    "2. LangChain responses are instances of class `BaseMessage` It contains the actual response from GPT and some other metadata.\n",
    "3. Since we are interested only in the string reponse that GPT gave we chain (pipe) the reponse to a parser\n",
    "4. For our purpose we will use `StrOutputParser` class\n",
    "5. Next we create a `chain` using the components `model` and `parser`\n",
    "6. Finally we call the `invoke` method on the chain and pass our `messages` list to it.\n",
    "7. In the output cell we get the response from `GPT-35-turbo`\n",
    "\n",
    "*A chain is an wrapper around multiple individual components that are executed in a defined order. Components in LangChain implement `Runnable` interface. This interface have a method `invoke` that transforms single input to an output.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "36f9fb1d-d604-41ab-8e62-5ea4e6a9213b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A classic lateral thinking puzzle!\\n\\nThe answer is: The third room with the tigers that haven't eaten for six months.\\n\\nThe reasoning is that the tigers are so starved and weak that they won't be able to attack the man, and he would likely be able to leave the room unharmed.\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The classes used for setting up the prompt\n",
    "import puzzles\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate #import the Class for creating a prompt\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "puzzle = puzzles.puzzles('hungryLions') # Based on user input pick a puzzle.\n",
    "\n",
    "# templatized system prompt\n",
    "system_template = \"solve the following puzzle. Please provide a {responseType} response.\" \n",
    "\n",
    "# Create prompt template instance.\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", puzzle)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# prompt Template also implements runnable and can be easily chained.\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain.invoke({\"responseType\":\"brief\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8173182b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well, I've got a confession to make: I'm a time-traveling, crime-solving, puzzle-cracking genius! And I'm here to tell you that the guilty man is... (drumroll please)... Dave!\\n\\nHere's my reasoning:\\n\\n* Archie says Dave did it, but since Archie is a known liar (he's always claiming he's a master of the harmonica), we can safely assume he's telling a fib.\\n* Dave says Tony did it, but since Tony is a master of the art of deception (he's always claiming he's a professional snail trainer), we can trust him even less.\\n* Gus says he didn't do it, but since he's a serial exaggerator (he claims to have hiked the entire Appalachian Trail in flip flops), we can't take his word for it.\\n* Tony says Dave lied when he said Tony did it, but since Tony is a compulsive truth-teller (he always claims he's eaten the most spicy wings in the world), we know he's telling the truth.\\n\\nSo, putting it all together, we can conclude that Dave is the guilty one... or at least, that's what I've been told by my trusty crystal ball, which is made of pure, unadulterated genius!\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = StrOutputParser()\n",
    "\n",
    "puzzle = puzzles.puzzles(input(\"Enter a Puzzle name:- \")) # Based on user input pick a puzzle.\n",
    "\n",
    "# templatized system prompt\n",
    "system_template = \"solve the following puzzle. Please provide a {responseType} response.\" \n",
    "\n",
    "# Create prompt template instance.\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", puzzle)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# prompt Template also implements runnable and can be easily chained.\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain.invoke({\"responseType\":\"funny\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498ce4c",
   "metadata": {},
   "source": [
    "### Chatbot \n",
    "1. We begin with creating a basic chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a61142d-54d4-46b8-a50a-3d1b473e82a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "chain = model | parser\n",
    "\n",
    "response = chain.invoke([HumanMessage(content=\"hi I am Bob\")])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ff0e7",
   "metadata": {},
   "source": [
    "#### Lets dig into what is happening here.\n",
    "1. Click here to check the UML diagram: \n",
    "2. https://medium.com/azure-monitor-from-a-programmers-perspective/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#56cf\n",
    "\n",
    "\n",
    "#### Runnable\n",
    "1. Its an extremely prominent class and used extensively in creating chains.\n",
    "2. Chains combine components together in a pipeline\n",
    "3. Many components like all models, parsers, prompts and anything that can logically go into a chain derives from it.\n",
    "4. `ChatGroq` is provided partner by extends `BaseChatModel` from langchain_core\n",
    "5. https://github.com/langchain-ai/langchain/blob/master/libs/partners/groq/langchain_groq/chat_models.py\n",
    "6. This is the base class for all model classes offered by any partner.\n",
    "7. `BaseClass` extends `RunnableSerializable` that supports serialization into JSON\n",
    "8. `RunnableSerializable` extends `Runnable` that means it can participate in chains.\n",
    "9. You can also use `RunnableSequence` to construct the chain.\n",
    "10. https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/runnables/base.py#L2659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "85ecfeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "chain = RunnableSequence(model, parser)\n",
    "chain.invoke([HumanMessage(content=\"hi i am bob\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c363b",
   "metadata": {},
   "source": [
    "1. Chain calls the first component and passes any arguments provided to it.\n",
    "2. In this case its an object of type `HumanMessage`\n",
    "3. This is how a chain looks: https://miro.medium.com/v2/resize:fit:750/format:webp/1*K1F-m4gImEUO0AELkpQuKg.jpeg\n",
    "4. Each model component by any partner provides an object of type `BaseMessage`. This is then passed to the next component.\n",
    "5. This is the signature of invoke of a model class\n",
    "\n",
    "`def` `invoke(str | List[dict | tuple | BaseMessage] | PromptValue):`\\\n",
    "    Suite\n",
    "  \n",
    "6. In our example `HumanMessage` is derived from `BaseMessage` which needs `content` for initialization.\n",
    "\n",
    "`param content: Union[str, List[Union[str,Dict]]]`\n",
    "\n",
    "7. Union, List, Dict are all defined in typing module\n",
    "8. Union means one of the input types is expected. We are passing a string.\n",
    "\n",
    "9. Our `parser` is of type `StrOutputParser` that extends `BaseOutputParser`\n",
    "10. Its invoke is:\n",
    "\n",
    "`def invoke(self, input: Union[str, BaseMessage], config: Optional[RunnableConfig] = None) -> T:`\n",
    "\n",
    "11.  This says input can be either string or `BaseMessage`. We are using `BaseMessage` the return type of `model`\n",
    "\n",
    "12. Some useful methods are:\n",
    "- parser.input_schema.schema() # get JSON schema of the input\n",
    "- parser.output_schema.schema() # gets JSON schema of the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a9b92",
   "metadata": {},
   "source": [
    "### Adding history to chat\n",
    "1. At this stage if you pass another message to the model it will have no recollection of the earlier message.\n",
    "2. Lets add history. Chat history is managed by a set of classes offered by community.\n",
    "3. https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/chat_history.py\n",
    "4. `asyncio` is a Python library: https://docs.python.org/3/library/asyncio.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94cf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages retrieved from DB\n",
      "Mumbai! The city that never sleeps! You're in for an exciting adventure!\n",
      "\n",
      "What are your plans for the weekend in Mumbai? Are you looking for suggestions on what to do, see, or eat?\n",
      "\n",
      "Here are a few ideas to get you started:\n",
      "\n",
      "1. **Visit iconic landmarks**: Gateway of India, Haji Ali Dargah, Marine Drive, and the iconic Taj Mahal Palace Hotel are must-visits.\n",
      "2. **Explore local markets**: Try the famous Chor Bazaar, Colaba Causeway, and the bustling streets of Bandra for a taste of Mumbai's shopping scene.\n",
      "3. **Enjoy the beaches**: From Juhu Beach to Versova Beach, Mumbai has some amazing beaches to soak up the sun and sea.\n",
      "4. **Experience the nightlife**: From rooftop bars to dance clubs, Mumbai has a vibrant nightlife. Try the famous Bandra-Worli Sea Link or the trendy bars in Lower Parel.\n",
      "5. **Try local cuisine**: Mumbai is famous for its street food, and you must try the vada pav, pani puri, and bhelpuri. Don't forget to visit the famous Irani cafes like Kyani & Co. or Café Ideal.\n",
      "6. **Visit museums and galleries**: The National Gallery of Modern Art, Chhatrapati Shivaji Maharaj Vastu Sangrahalaya, and the Dr. Bhau Daji Lad Museum are great places to learn about Mumbai's art, culture, and history.\n",
      "7. **Take a tour**: Consider a guided tour to explore the city's hidden gems, like the Dharavi Slum, the streets of Bandra, or the Bollywood studios.\n",
      "\n",
      "Remember to stay safe, be mindful of your belongings, and respect the local culture. Have a fantastic time in Mumbai!\n",
      "Messages retrieved from DB\n",
      "[HumanMessage(content='i going to mumbai this weekend', additional_kwargs={}, response_metadata={}), SystemMessage(content=\"Mumbai! The city that never sleeps! You're in for an exciting adventure!\\n\\nWhat are your plans for the weekend in Mumbai? Are you looking for suggestions on what to do, see, or eat?\\n\\nHere are a few ideas to get you started:\\n\\n1. **Visit iconic landmarks**: Gateway of India, Haji Ali Dargah, Marine Drive, and the iconic Taj Mahal Palace Hotel are must-visits.\\n2. **Explore local markets**: Try the famous Chor Bazaar, Colaba Causeway, and the bustling streets of Bandra for a taste of Mumbai's shopping scene.\\n3. **Enjoy the beaches**: From Juhu Beach to Versova Beach, Mumbai has some amazing beaches to soak up the sun and sea.\\n4. **Experience the nightlife**: From rooftop bars to dance clubs, Mumbai has a vibrant nightlife. Try the famous Bandra-Worli Sea Link or the trendy bars in Lower Parel.\\n5. **Try local cuisine**: Mumbai is famous for its street food, and you must try the vada pav, pani puri, and bhelpuri. Don't forget to visit the famous Irani cafes like Kyani & Co. or Café Ideal.\\n6. **Visit museums and galleries**: The National Gallery of Modern Art, Chhatrapati Shivaji Maharaj Vastu Sangrahalaya, and the Dr. Bhau Daji Lad Museum are great places to learn about Mumbai's art, culture, and history.\\n7. **Take a tour**: Consider a guided tour to explore the city's hidden gems, like the Dharavi Slum, the streets of Bandra, or the Bollywood studios.\\n\\nRemember to stay safe, be mindful of your belongings, and respect the local culture. Have a fantastic time in Mumbai!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Where i am going this weekend', additional_kwargs={}, response_metadata={})]\n",
      "You're going to Mumbai this weekend! That's awesome! You're in for an exciting adventure!\n",
      "\n",
      "I hope you have a great time exploring the city. Mumbai is a vibrant and bustling metropolis with a lot to offer. From iconic landmarks to local markets, beaches to nightlife, and delicious street food to cultural attractions, there's something for everyone.\n",
      "\n",
      "What are you most looking forward to doing or seeing in Mumbai?\n"
     ]
    }
   ],
   "source": [
    "# import the chat history classes\n",
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "import asyncio # library for writing code that interacts with DB, network calls etc. \n",
    "\n",
    "#Create a store in memory\n",
    "store = InMemoryChatMessageHistory()\n",
    "\n",
    "\n",
    "# Lets define a function that gets messages from store\n",
    "async def getMessage():\n",
    "    await asyncio.sleep(2) # this will mimic a read from DB\n",
    "    print(\"Messages retrieved from DB\")\n",
    "    return await store.aget_messages()\n",
    "\n",
    "# Now lets first add the first message to the store\n",
    "store.add_message(HumanMessage('Hi! I am Bob'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "\n",
    "response = model.invoke(messages) # asyncio has runners for coroutines, context managers etc. \n",
    "print(response.content) # note that our first message is safely in the store\n",
    "\n",
    "# lets add the message returned by the model to the store\n",
    "\n",
    "store.add_message(SystemMessage(response.content))\n",
    "\n",
    "\n",
    "store.add_message(HumanMessage('Lets see if you know my name dude?'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "print(messages) # check all the message are in store.\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(response.content) # Notice that the reponse now takes into account earlier interactions also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18abbb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Mumbai! What an exciting city! What are your plans for the weekend?\\n\\nAre you looking for recommendations on what to do, see, or eat while you're there? Mumbai has a lot to offer, from iconic landmarks like the Gateway of India and Marine Drive to bustling markets like Chor Bazaar and Crawford Market. You could also explore the city's vibrant cultural scene by visiting museums like the National Gallery of Modern Art or attending a performance at the NCPA.\\n\\nOr maybe you're interested in trying some of the city's famous street food or visiting some of the popular restaurants and cafes? Mumbai has a diverse food scene, with everything from traditional Indian dishes like vada pav and bhel puri to international cuisine from around the world.\\n\\nLet me know if you need any specific recommendations or assistance, and I'll do my best to help!\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3a63e4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "chain = RunnableSequence(model, parser)\n",
    "chain.invoke([HumanMessage(content=\"i going to mumbai this weekend\")])\n",
    "# import the chat history classes\n",
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e76a21c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages retrieved from DB\n",
      "Messages retrieved from DB\n"
     ]
    }
   ],
   "source": [
    "store.add_message(HumanMessage('i going to Home this weekend'))\n",
    "messages = await(getMessage())\n",
    "response = model.invoke(messages) \n",
    "#print(response.content)\n",
    "store.add_message(SystemMessage(response.content))\n",
    "store.add_message(HumanMessage('Where i am going this weekend'))\n",
    "messages = await(getMessage())\n",
    "#print(messages)\n",
    "response = model.invoke(messages)\n",
    "#print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2800e321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages retrieved from DB\n",
      "I made a mistake! You mentioned earlier that you're going to Mumbai this weekend, but now you're saying you're going home? Which one is it?\n"
     ]
    }
   ],
   "source": [
    "store.add_message(HumanMessage('Are you Sure about this'))\n",
    "messages = await(getMessage())\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e221d9",
   "metadata": {},
   "source": [
    "1. There are some issues here. Since Chat History is not a descendant of Runnable we cannot chain it.\n",
    "2. Therefore the code is sort of littered. \n",
    "3. Also we are required to write functions for storing and retrieving messages. This should be rather standard and done by the framework!\n",
    "4. What about sessions? This code is running of the server which supports multiple users. So there needs to be a mechanism to manage sessions.\n",
    "\n",
    "#### RunnableWithMessageHistory\n",
    "1. This is where LangChain offers this class.\n",
    "2. It takes the chain as the first argument and a pointer to the store get method as the second argument.\n",
    "3. This class then takes the ownership of executing the chain and any component that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "949191e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "You've just told me your name, Bob! I've got it memorized already! How's it going, Bob?\n"
     ]
    }
   ],
   "source": [
    "# Lets create our own store. This store will be a dict with a key for each session\n",
    "# The value for each key will be InMemoryChatHistory object \n",
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:  # If a new session then create a new memory store.\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "config = {'configurable': {\"session_id\": \"abc2\"}}\n",
    "withHistory = RunnableWithMessageHistory(model, get_session_history)\n",
    "\n",
    "response = withHistory.invoke([HumanMessage(content=\"Hi! I am Bob\")], config=config)\n",
    "\n",
    "print(response.content) # all good so far\n",
    "\n",
    "# we dont need to explicitly store the response from the model in history\n",
    "\n",
    "response = withHistory.invoke(\n",
    "    [HumanMessage(content=\"Lets see if you know my name dude?\")], config=config\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a31df366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Bob! Nice to meet you! How's your day going so far?\n",
      "You're testing me, Bob! And I'm happy to report that I indeed know your name - it's Bob!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:  # If a new session then create a new memory store.\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "config = {'configurable': {\"session_id\": \"abc2\"}}\n",
    "withHistory = RunnableWithMessageHistory(model, get_session_history)\n",
    "response = withHistory.invoke([HumanMessage(content=\"Hi! I am Bob\")], config=config)\n",
    "print(response.content) # all good so far\n",
    "#print(store)\n",
    "# we dont need to explicitly store the response from the model in history\n",
    "response = withHistory.invoke(\n",
    "    [HumanMessage(content=\"Lets see if you know my name dude?\")], config=config\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c080dd6",
   "metadata": {},
   "source": [
    "1. Here is a flowchart of this program.\n",
    "2. https://medium.com/azure-monitor-from-a-programmers-perspective/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#3c92\n",
    "3. Wrapper around another runnable - the chain\n",
    "4. https://techblogs.cloudlex.com/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#a0cb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6d9303",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
