{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6e9013",
   "metadata": {},
   "source": [
    "### Setup Langchain + LLM\n",
    "1. Install Langchain: \n",
    "- pip intall langchain\n",
    "2. Install integration packages.\n",
    "- pip install -U langchain-cohere\n",
    "- pip install -U langchain-groq\n",
    "- pip install -U langchain-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ae4cdab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hi there! I can help with that. \\n\\nPlease provide me with your location, and I will give you the current weather conditions and forecast for the day. \\n\\n- Have a great day' additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '0bbd7c85-d588-46b5-9123-8e4575472744', 'token_count': {'input_tokens': 232.0, 'output_tokens': 40.0}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': '0bbd7c85-d588-46b5-9123-8e4575472744', 'token_count': {'input_tokens': 232.0, 'output_tokens': 40.0}} id='run-4a4d7a12-a3d8-4df6-91d8-acfd47e89b2f-0' usage_metadata={'input_tokens': 232, 'output_tokens': 40, 'total_tokens': 272}\n",
      "content=\"I'd be happy to help you with that!\\n\\nAccording to our latest forecast, today's weather is looking partly cloudy with a high chance of scattered showers throughout the day. The temperature is expected to reach a comfortable 22°C (72°F), with a gentle breeze blowing at about 15 km/h (9 mph).\\n\\nIn terms of precipitation, we're expecting light to moderate rain showers, with an estimated total accumulation of around 5-10 mm (0.2-0.4 inches). However, the rain should clear out by late afternoon, giving way to partial sunshine and a pleasant evening.\\n\\nPlease keep in mind that this is just a general forecast, and actual weather conditions may vary. If you have any specific concerns or need more detailed information, feel free to ask!\\n\\nHave a great day!\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 163, 'prompt_tokens': 51, 'total_tokens': 214, 'completion_time': 0.135833333, 'prompt_time': 0.002063437, 'queue_time': 0.012366301, 'total_time': 0.13789677}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None} id='run-613a0f1c-8f5d-41c0-8529-37677fd35b31-0' usage_metadata={'input_tokens': 51, 'output_tokens': 163, 'total_tokens': 214}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../config.ini')\n",
    "groq = config['groq']\n",
    "cohere = config['cohere']\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = groq.get('GROQ_API_KEY')\n",
    "os.environ['COHERE_API_KEY'] = cohere.get('COHERE_API_KEY')\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are a weather service. You will respond to weather queries to the best of you ability. You will always end with - Have a great day'),\n",
    "    HumanMessage(content='Hey whats the weather like today?')\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "## code for cohere.\n",
    "model = ChatCohere(model=\"command-r-plus\")\n",
    "print(model.invoke(messages))\n",
    "\n",
    "## Code for Groq\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "print(model.invoke(messages))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f21a0672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hi there! I can help with that. \\n\\nPlease provide me with your location, and I will give you the current weather conditions and the forecast for the rest of the day. \\n\\n- Have a great day!' additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'd11fcb9c-925e-481e-944d-7749163eba97', 'token_count': {'input_tokens': 232.0, 'output_tokens': 45.0}} response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'd11fcb9c-925e-481e-944d-7749163eba97', 'token_count': {'input_tokens': 232.0, 'output_tokens': 45.0}} id='run-8212d5d1-06a2-4833-8271-0f370caa0c02-0' usage_metadata={'input_tokens': 232, 'output_tokens': 45, 'total_tokens': 277}\n",
      "content=\"I'd be happy to help! According to our latest forecast, today's weather is looking quite pleasant! There's a gentle breeze blowing at about 5 mph, and the skies are mostly clear with a few puffy clouds scattered around. The temperature is expected to reach a high of around 72°F (22°C) with a low of 55°F (13°C) overnight.\\n\\nIf you're planning on spending some time outdoors, you might want to pack a light jacket for your morning commute, but by midday, it should be quite comfortable to enjoy the sunshine!\\n\\nHave a great day!\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 122, 'prompt_tokens': 51, 'total_tokens': 173, 'completion_time': 0.101666667, 'prompt_time': 0.002643873, 'queue_time': 0.011807187, 'total_time': 0.10431054}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None} id='run-439b288d-d097-4633-9134-d0957c30cebe-0' usage_metadata={'input_tokens': 51, 'output_tokens': 122, 'total_tokens': 173}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content='You are a weather service. You will respond to weather queries to the best of you ability. You will always end with - Have a great day'),\n",
    "    HumanMessage(content='Hey whats the weather like today?')\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "## code for cohere.\n",
    "model = ChatCohere(model=\"command-r-plus\")\n",
    "print(model.invoke(messages))\n",
    "\n",
    "## Code for Groq\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "print(model.invoke(messages))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76fa0d3-c23a-455f-998f-bb68018d0db0",
   "metadata": {},
   "source": [
    "## Create the prompt\n",
    "1. Imports Human and System message classes. System represents our instructions to GPT and Human represents the question or prompt that the user provides.\n",
    "2. LangChain responses are instances of class `BaseMessage` It contains the actual response from GPT and some other metadata.\n",
    "3. Since we are interested only in the string reponse that GPT gave we chain (pipe) the reponse to a parser\n",
    "4. For our purpose we will use `StrOutputParser` class\n",
    "5. Next we create a `chain` using the components `model` and `parser`\n",
    "6. Finally we call the `invoke` method on the chain and pass our `messages` list to it.\n",
    "7. In the output cell we get the response from `GPT-35-turbo`\n",
    "\n",
    "*A chain is an wrapper around multiple individual components that are executed in a defined order. Components in LangChain implement `Runnable` interface. This interface have a method `invoke` that transforms single input to an output.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36f9fb1d-d604-41ab-8e62-5ea4e6a9213b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<cohere.client.Client object at 0x706ed6b600e0> async_client=<cohere.client.AsyncClient object at 0x706ed6b61c10> model='command-r' cohere_api_key=SecretStr('**********')\n",
      "['/usr/local/python/3.12.1/lib/python312.zip', '/usr/local/python/3.12.1/lib/python3.12', '/usr/local/python/3.12.1/lib/python3.12/lib-dynload', '', '/home/codespace/.local/lib/python3.12/site-packages', '/usr/local/python/3.12.1/lib/python3.12/site-packages']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A classic lateral thinking puzzle!\\n\\nThe man should choose the third room. Why? Because the tigers haven't eaten for six months, which means they're likely to be dead or too weak to attack him. It's the safest option!\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The classes used for setting up the prompt\n",
    "import puzzles\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate #import the Class for creating a prompt\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "puzzle = puzzles.puzzles('hungryLions') # Based on user input pick a puzzle.\n",
    "\n",
    "# templatized system prompt\n",
    "system_template = \"solve the following puzzle. Please provide a {responseType} response.\" \n",
    "\n",
    "# Create prompt template instance.\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", puzzle)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# prompt Template also implements runnable and can be easily chained.\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain.invoke({\"responseType\":\"brief\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8173182b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The classic \"Who\\'s the crook?\" puzzle!\\n\\nAfter analyzing the statements, I\\'ve come to a shocking conclusion:\\n\\nThe guilty man is... Archie!\\n\\nWhy, you ask? Well, it\\'s because Archie is the only one who didn\\'t directly implicate himself as the criminal. He said \"Dave did it,\" which means Archie is trying to shift the blame onto Dave. But here\\'s the thing: if Archie wasn\\'t the guilty one, he wouldn\\'t need to lie about Dave\\'s guilt. It\\'s like he\\'s trying to cover his own tracks by pointing fingers elsewhere. And if he was innocent, he would\\'ve simply said \"I didn\\'t do it\" like Gus did.\\n\\nSo, Archie\\'s statement is the only one that could be a lie, making him the guilty party!\\n\\nOf course, this is just a ridiculous theory, but hey, it\\'s a fun way to solve the puzzle, right?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = StrOutputParser()\n",
    "\n",
    "puzzle = puzzles.puzzles(input(\"Enter a Puzzle name:- \")) # Based on user input pick a puzzle.\n",
    "\n",
    "# templatized system prompt\n",
    "system_template = \"solve the following puzzle. Please provide a {responseType} response.\" \n",
    "\n",
    "# Create prompt template instance.\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", puzzle)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# prompt Template also implements runnable and can be easily chained.\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain.invoke({\"responseType\":\"funny\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9498ce4c",
   "metadata": {},
   "source": [
    "### Chatbot \n",
    "1. We begin with creating a basic chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a61142d-54d4-46b8-a50a-3d1b473e82a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "chain = model | parser\n",
    "\n",
    "response = chain.invoke([HumanMessage(content=\"hi I am Bob\")])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ff0e7",
   "metadata": {},
   "source": [
    "#### Lets dig into what is happening here.\n",
    "1. Click here to check the UML diagram: \n",
    "2. https://medium.com/azure-monitor-from-a-programmers-perspective/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#56cf\n",
    "\n",
    "\n",
    "#### Runnable\n",
    "1. Its an extremely prominent class and used extensively in creating chains.\n",
    "2. Chains combine components together in a pipeline\n",
    "3. Many components like all models, parsers, prompts and anything that can logically go into a chain derives from it.\n",
    "4. `ChatGroq` is provided partner by extends `BaseChatModel` from langchain_core\n",
    "5. https://github.com/langchain-ai/langchain/blob/master/libs/partners/groq/langchain_groq/chat_models.py\n",
    "6. This is the base class for all model classes offered by any partner.\n",
    "7. `BaseClass` extends `RunnableSerializable` that supports serialization into JSON\n",
    "8. `RunnableSerializable` extends `Runnable` that means it can participate in chains.\n",
    "9. You can also use `RunnableSequence` to construct the chain.\n",
    "10. https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/runnables/base.py#L2659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85ecfeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "chain = RunnableSequence(model, parser)\n",
    "chain.invoke([HumanMessage(content=\"hi i am bob\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c363b",
   "metadata": {},
   "source": [
    "1. Chain calls the first component and passes any arguments provided to it.\n",
    "2. In this case its an object of type `HumanMessage`\n",
    "3. This is how a chain looks: https://miro.medium.com/v2/resize:fit:750/format:webp/1*K1F-m4gImEUO0AELkpQuKg.jpeg\n",
    "4. Each model component by any partner provides an object of type `BaseMessage`. This is then passed to the next component.\n",
    "5. This is the signature of invoke of a model class\n",
    "\n",
    "`def` `invoke(str | List[dict | tuple | BaseMessage] | PromptValue):`\\\n",
    "    Suite\n",
    "  \n",
    "6. In our example `HumanMessage` is derived from `BaseMessage` which needs `content` for initialization.\n",
    "\n",
    "`param content: Union[str, List[Union[str,Dict]]]`\n",
    "\n",
    "7. Union, List, Dict are all defined in typing module\n",
    "8. Union means one of the input types is expected. We are passing a string.\n",
    "\n",
    "9. Our `parser` is of type `StrOutputParser` that extends `BaseOutputParser`\n",
    "10. Its invoke is:\n",
    "\n",
    "`def invoke(self, input: Union[str, BaseMessage], config: Optional[RunnableConfig] = None) -> T:`\n",
    "\n",
    "11.  This says input can be either string or `BaseMessage`. We are using `BaseMessage` the return type of `model`\n",
    "\n",
    "12. Some useful methods are:\n",
    "- parser.input_schema.schema() # get JSON schema of the input\n",
    "- parser.output_schema.schema() # gets JSON schema of the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a9b92",
   "metadata": {},
   "source": [
    "### Adding history to chat\n",
    "1. At this stage if you pass another message to the model it will have no recollection of the earlier message.\n",
    "2. Lets add history. Chat history is managed by a set of classes offered by community.\n",
    "3. https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/chat_history.py\n",
    "4. `asyncio` is a Python library: https://docs.python.org/3/library/asyncio.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c94cf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages retrieved from DB\n",
      "Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "Messages retrieved from DB\n",
      "[HumanMessage(content='Hi! I am Bob', additional_kwargs={}, response_metadata={}), SystemMessage(content=\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Lets see if you know my name dude?', additional_kwargs={}, response_metadata={})]\n",
      "Yeah! I knew it! Your name is Bob, right?\n"
     ]
    }
   ],
   "source": [
    "# import the chat history classes\n",
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "import asyncio # library for writing code that interacts with DB, network calls etc. \n",
    "\n",
    "#Create a store in memory\n",
    "store = InMemoryChatMessageHistory()\n",
    "\n",
    "\n",
    "# Lets define a function that gets messages from store\n",
    "async def getMessage():\n",
    "    await asyncio.sleep(2) # this will mimic a read from DB\n",
    "    print(\"Messages retrieved from DB\")\n",
    "    return await store.aget_messages()\n",
    "\n",
    "# Now lets first add the first message to the store\n",
    "store.add_message(HumanMessage('Hi! I am Bob'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "\n",
    "response = model.invoke(messages) # asyncio has runners for coroutines, context managers etc. \n",
    "print(response.content) # note that our first message is safely in the store\n",
    "\n",
    "# lets add the message returned by the model to the store\n",
    "\n",
    "store.add_message(SystemMessage(response.content))\n",
    "\n",
    "\n",
    "store.add_message(HumanMessage('Lets see if you know my name dude?'))\n",
    "\n",
    "messages = await(getMessage())\n",
    "\n",
    "print(messages) # check all the message are in store.\n",
    "\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(response.content) # Notice that the reponse now takes into account earlier interactions also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18abbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a63e4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "chain = RunnableSequence(model, parser)\n",
    "chain.invoke([HumanMessage(content=\"i going to mumbai this weekend\")])\n",
    "# import the chat history classes\n",
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e76a21c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages retrieved from DB\n",
      "Messages retrieved from DB\n"
     ]
    }
   ],
   "source": [
    "store.add_message(HumanMessage('i going to Home this weekend'))\n",
    "messages = await(getMessage())\n",
    "response = model.invoke(messages) \n",
    "#print(response.content)\n",
    "store.add_message(SystemMessage(response.content))\n",
    "store.add_message(HumanMessage('Where i am going this weekend'))\n",
    "messages = await(getMessage())\n",
    "#print(messages)\n",
    "response = model.invoke(messages)\n",
    "#print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2800e321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages retrieved from DB\n",
      "I apologize, I made a mistake! You didn't tell me where you're going, you just said \"Home\" which could mean many things!\n"
     ]
    }
   ],
   "source": [
    "store.add_message(HumanMessage('Are you Sure about this'))\n",
    "messages = await(getMessage())\n",
    "response = model.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e221d9",
   "metadata": {},
   "source": [
    "1. There are some issues here. Since Chat History is not a descendant of Runnable we cannot chain it.\n",
    "2. Therefore the code is sort of littered. \n",
    "3. Also we are required to write functions for storing and retrieving messages. This should be rather standard and done by the framework!\n",
    "4. What about sessions? This code is running of the server which supports multiple users. So there needs to be a mechanism to manage sessions.\n",
    "\n",
    "#### RunnableWithMessageHistory\n",
    "1. This is where LangChain offers this class.\n",
    "2. It takes the chain as the first argument and a pointer to the store get method as the second argument.\n",
    "3. This class then takes the ownership of executing the chain and any component that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "949191e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "You're testing my skills, Bob! And... yes, I know your name is Bob!\n"
     ]
    }
   ],
   "source": [
    "# Lets create our own store. This store will be a dict with a key for each session\n",
    "# The value for each key will be InMemoryChatHistory object \n",
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:  # If a new session then create a new memory store.\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "config = {'configurable': {\"session_id\": \"abc2\"}}\n",
    "withHistory = RunnableWithMessageHistory(model, get_session_history)\n",
    "\n",
    "response = withHistory.invoke([HumanMessage(content=\"Hi! I am Bob\")], config=config)\n",
    "\n",
    "print(response.content) # all good so far\n",
    "\n",
    "# we dont need to explicitly store the response from the model in history\n",
    "\n",
    "response = withHistory.invoke(\n",
    "    [HumanMessage(content=\"Lets see if you know my name dude?\")], config=config\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a31df366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\n",
      "You've just introduced yourself as Bob, nice and clear! I'm keeping track, buddy.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:  # If a new session then create a new memory store.\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "config = {'configurable': {\"session_id\": \"abc2\"}}\n",
    "withHistory = RunnableWithMessageHistory(model, get_session_history)\n",
    "response = withHistory.invoke([HumanMessage(content=\"Hi! I am Bob\")], config=config)\n",
    "print(response.content) # all good so far\n",
    "#print(store)\n",
    "# we dont need to explicitly store the response from the model in history\n",
    "response = withHistory.invoke(\n",
    "    [HumanMessage(content=\"Lets see if you know my name dude?\")], config=config\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c080dd6",
   "metadata": {},
   "source": [
    "1. Here is a flowchart of this program.\n",
    "2. https://medium.com/azure-monitor-from-a-programmers-perspective/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#3c92\n",
    "3. Wrapper around another runnable - the chain\n",
    "4. https://techblogs.cloudlex.com/langchain-ii-basic-chatbot-unpacked-a60510b9ac6b#a0cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "060056f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranveer Singh\n",
      "Marrakech\n",
      "Space Opera\n",
      "**Movie Title:** \"Cosmic Odyssey: The Marrakech Quest\"\n",
      "\n",
      "**Genre:** Space Opera\n",
      "\n",
      "**Starring:** Ranveer Singh\n",
      "\n",
      "**Story:**\n",
      "\n",
      "In the bustling city of Marrakech, a young and ambitious astronaut, Captain Zafar (Ranveer Singh), has been recruited by the prestigious Space Agency of the United Arab Emirates to lead a mission to explore a newly discovered planet on the outskirts of the solar system.\n",
      "\n",
      "As Zafar and his team of experts embark on their perilous journey, they are faced with numerous challenges, including treacherous asteroid fields, hostile alien encounters, and malfunctioning spacecraft equipment. Despite these obstacles, Zafar's unwavering determination and leadership skills help his team to overcome each hurdle, bringing them closer to their ultimate goal of reaching the mysterious planet.\n",
      "\n",
      "Upon arrival, the team discovers a strange, glowing portal on the planet's surface, emitting an otherworldly energy signal. Intrigued, Zafar decides to investigate further, and as he approaches the portal, he is suddenly transported to a futuristic city on a distant planet, where he meets a beautiful and enigmatic alien princess named Lila.\n",
      "\n",
      "Lila reveals to Zafar that her planet is facing a catastrophic threat from an evil alien warlord, who seeks to destroy her world and claim its resources for himself. She asks Zafar to help her rally the resistance forces and defeat the warlord in a desperate battle to save her planet.\n",
      "\n",
      "With his astronaut training and quick thinking, Zafar agrees to help Lila, and together they embark on a thrilling adventure to unite the warring factions and defeat the warlord. Along the way, Zafar and Lila develop a strong bond, and the astronaut finds himself torn between his duty to his mission and his growing feelings for the alien princess.\n",
      "\n",
      "As the stakes grow higher, Zafar and his new allies face off against the warlord's forces in an epic battle that will determine the fate of both Lila's planet and Earth. Will Zafar be able to save the day and return to Marrakech a hero, or will his quest for adventure and love lead him down a path of destruction? Find out in \"Cosmic Odyssey: The Marrakech Quest,\" starring Ranveer Singh.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import configparser\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_groq import ChatGroq\n",
    "# from langchain_cohere import ChatCohere\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# config = configparser.ConfigParser()\n",
    "# config.read('config.ini')\n",
    "# groq = config['groq']\n",
    "# cohere = config['cohere']\n",
    "# os.environ['GROQ_API_KEY'] = groq.get('GROQ_API_KEY')\n",
    "# os.environ['COHERE_API_KEY'] = cohere.get('COHERE_API_KEY')\n",
    "\n",
    "def actor_picker():\n",
    "    messages = [\n",
    "    SystemMessage(content='You have to pickup any random Bollywood actor from 1 to 5 and only give a actor name'),\n",
    "    HumanMessage(content=input(\"Enter a number from 1 to 5 choose a random Bollywood actor :- \"))\n",
    "    ]\n",
    "    parser = StrOutputParser()\n",
    "    model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "    chain=model | parser\n",
    "    response= chain.invoke(messages)\n",
    "    print(response)\n",
    "    return response\n",
    "def location_picker():\n",
    "    messages = [\n",
    "    SystemMessage(content='You have to pick any random location and give different location and only give a location name'),\n",
    "    #HumanMessage(content=input(\"Enter a number from 1 to 5 choose a random location:- \"))\n",
    "    ]\n",
    "    parser = StrOutputParser()\n",
    "    model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "    chain=model | parser\n",
    "    response= chain.invoke(messages)\n",
    "    print(response)\n",
    "    return response\n",
    "def theme_picker():\n",
    "    messages = [\n",
    "    SystemMessage(content='You have to pickup any random theme for movie and only give a theme'),\n",
    "    #HumanMessage(content=input(\"Enter a number from 1 to 5 choose a random theme:- \"))\n",
    "    ]\n",
    "    parser = StrOutputParser()\n",
    "    model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "    chain=model | parser\n",
    "    response= chain.invoke(messages)\n",
    "    print(response)\n",
    "    return response\n",
    "actor=actor_picker()\n",
    "location=location_picker()\n",
    "theme=theme_picker()\n",
    "parser = StrOutputParser()\n",
    "\n",
    "system_template = \"Return an output in string form as a story reading the output for the input three functions. Please provide a {{size}} related response.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template),\n",
    "        (\"user\", 'Create a story for a movie in which the actor is {actor}, the location for movie is {location} and the theme for the movie is {theme}.Tell the genre, location and starring for the movie and then display story.')  # Use string representation of my_list here\n",
    "    ]\n",
    ")\n",
    "def scriptwriter():\n",
    "    model_groq = ChatGroq(model=\"llama3-8b-8192\")\n",
    "    chain = prompt_template | model_groq | parser\n",
    "\n",
    "    final_story_output = chain.invoke({\"actor\":actor,\"location\":location,\"theme\":theme})\n",
    "\n",
    "    print(final_story_output)\n",
    "    return final_story_output\n",
    "script=scriptwriter()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6d9303",
   "metadata": {},
   "source": [
    "if want to change in tha script there will be option below\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8365e080",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m History_Store()\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m New_Story()\n\u001b[0;32m---> 29\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/asyncio/runners.py:190\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug, loop_factory)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug, loop_factory\u001b[38;5;241m=\u001b[39mloop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "from langchain_core.chat_history import(InMemoryChatMessageHistory)\n",
    "import asyncio\n",
    "store=InMemoryChatMessageHistory()\n",
    "\n",
    "async def History_Store():\n",
    "    store.add_message(HumanMessage(content=f'{script}'))\n",
    "    messages = await store.aget_messages()\n",
    "    response = model.invoke(messages)\n",
    "    print(response.content)\n",
    "    store.add_message(SystemMessage(content=response.content))\n",
    "    \n",
    "async def New_Story():\n",
    "    await asyncio.sleep(2)  \n",
    "    system_message_content = \"Please change the movie story according to the changes in the given input\"\n",
    "    store.add_message(SystemMessage(content=system_message_content))\n",
    "    user_input = input(\"Tell if you want to make changes to in your story:- \")\n",
    "    store.add_message(HumanMessage(content=user_input))\n",
    "    messages = await store.aget_messages()\n",
    "    response = model.invoke(messages)\n",
    "    print(\"Model Response:\", response.content)\n",
    "    store.add_message(SystemMessage(content=response.content))\n",
    "\n",
    "async def main():\n",
    "    await History_Store()\n",
    "    await New_Story()\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
